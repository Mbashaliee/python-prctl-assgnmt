#writing a function
def count_lines_and_words(filename):
    
    with open('filename', 'r') as file:
    
        lines = file.readlines()
    
        line_count = len(lines)
    
        word_count = sum(len(line.split()) for line in lines)
    
    return line_count, word_count

files = [
    
    'data/obama_speech.txt',
    
    'data/michelle_obama_speech.txt',
    
    'data/donald_speech.txt',
    
    'data/melina_trump_speech.txt'
]

for file in files:

    line_count, word_count = count_lines_and_words(file)

    print(f"File: {file}")

    print(f"Number of lines: {line_count}")

    print(f"Number of words: {word_count}")

    print()


#Read the countries_data.json data file in data directory, create a function that finds the ten most spoken languages
import json

from collections import Counter

def most_spoken_languages(filename, n):

    with open(filename, 'r') as file:

        languages=[country['Language'] for country in data]

        all_languages=[language for sublist in language for language in sublist]

        top_languages=language_counts.most_common(n)

        return top_languages
    

print(most_spoken_languages(filename='./data/countries_data.json, n=10'))

print(most_spoken_languages(filename='.data/countries_data.json, n=3'))


#User Read the countries_data.json data file in data directory, create a function that creates a list of the ten most populated countries

import json

def most_populated_countries(filename, n):
    
    with open(filename, 'r') as file:
    
        data = json.load(file)
    
    countries = [{'country': country['name'], 'population': country['population']} for country in data]
    
    sorted_countries = sorted(countries, key=lambda x: x['population'], reverse=True)
    
    top_countries = sorted_countries[:n]
    
    return top_countries

print(most_populated_countries(filename='./data/countries_data.json', n=10))

print(most_populated_countries(filename='./data/countries_data.json', n=3))



##Extract all incoming email addresses as a list from the email_exchange_big.txt file.
import re

def extract_email_addresses(filename):

    with open(filename, 'r') as file:

        text = file.read()
    
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    
    email_addresses = re.findall(pattern, text)
    
    return email_addresses

email_addresses = extract_email_addresses(filename='email_exchange_big.txt')

print(email_addresses[:10])  # Print the first 10 email addresses



#from collections import Counter
import re

def find_most_common_words(text, n):
    
    if isinstance(text, str) and text.endswith('.txt'):
        
        with open(text, 'r') as file:

            text = file.read()

    words = re.findall(r'\b\w+\b', text.lower())
    
    word_counts = Counter(words)
    
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_words[:n]

print(find_most_common_words('sample.txt', 10))

print(find_most_common_words('sample.txt', 5))


#Use the function, find_most_frequent_words to find: a) The ten most frequent words used in Obama's speech b) The ten most frequent words used in Michelle's speech c) The ten most frequent words used in Trump's speech d) The ten most frequent words used in Melina's speech

def find_most_common_words(text, n):
    
    if isinstance(text, str) and text.endswith('.txt'):
        
        with open(text, 'r') as file:

            text = file.read()
    
    words = re.findall(r'\b\w+\b', text.lower())
    
    word_counts = Counter(words)
    
    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_words[:n]

obama_words = find_most_common_words('obama_speech.txt', 10)

michelle_words = find_most_common_words('michelle_obama_speech.txt', 10)

trump_words = find_most_common_words('trump_speech.txt', 10)

melania_words = find_most_common_words('melania_trump_speech.txt', 10)

print("Obama's speech:", obama_words)

print("Michelle's speech:", michelle_words)

print("Trump's speech:", trump_words)

print("Melania's speech:", melania_words)



#
import re

from collections import Counter

def clean_text(text):

    return re.sub(r'\W+', ' ', text.lower())

def remove_support_words(text, stop_words):

    return ' '.join([word for word in text.split() if word not in stop_words])

def check_text_similarity(text1, text2):

    set1 = set(text1.split())

    set2 = set(text2.split())

    intersection = set1.intersection(set2)

    union = set1.union(set2)

    return len(intersection) / len(union)

with open('data/stop_words.txt', 'r') as file:

    stop_words = file.read().splitlines()

with open('data/romeo_and_juliet.txt', 'r') as file:

    romeo_text = file.read()

cleaned_romeo_text = clean_text(romeo_text)

cleaned_romeo_text_without_stop_words = remove_support_words(cleaned_romeo_text, stop_words)

word_counts = Counter(cleaned_romeo_text_without_stop_words.split())

most_repeated_words = word_counts.most_common(10)

print("10 most repeated words in Romeo and Juliet:")

for word, count in most_repeated_words:

    print(f"{word}: {count}")

with open('data/michelle_obama_speech.txt', 'r') as file:

    michelle_speech = file.read()

with open('data/melania_trump_speech.txt', 'r') as file:

    melania_speech = file.read()

cleaned_michelle_speech = clean_text(michelle_speech)

cleaned_melania_speech = clean_text(melania_speech)

similarity = check_text_similarity(cleaned_michelle_speech, cleaned_melania_speech)

print(f"Similarity between Michelle Obama's and Melania Trump's speeches: {similarity}")
